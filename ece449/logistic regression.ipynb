{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d63197f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: w [-0.55178498 -0.91364436], loss 1.1628\n",
      "Iter 0: w [-0.55178498 -0.91364436], loss 1.1628\n",
      "Iter 0: w [-0.55178498 -0.91364436], loss 1.1628\n",
      "Iter 0: w [-0.55178498 -0.91364436], loss 1.1628\n",
      "Iter 1: w [-0.16478594 -0.81641667], loss 0.8714\n",
      "Iter 2: w [ 0.1485189 -0.719854 ], loss 0.6801\n",
      "Iter 3: w [ 0.38793539 -0.63542287], loss 0.5666\n",
      "Iter 4: w [ 0.56853115 -0.56592831], loss 0.5001\n",
      "Iter 5: w [ 0.70871398 -0.50874911], loss 0.4587\n",
      "Iter 6: w [ 0.82165737 -0.46081005], loss 0.4311\n",
      "Iter 7: w [ 0.915578   -0.41983206], loss 0.4117\n",
      "Iter 8: w [ 0.99562034 -0.38423734], loss 0.3973\n",
      "Iter 9: w [ 1.06513626 -0.35292041], loss 0.3864\n",
      "Iter 10: w [ 1.12640724 -0.32508377], loss 0.3778\n",
      "Iter 11: w [ 1.18104861 -0.30013449], loss 0.3710\n",
      "Iter 12: w [ 1.23024331 -0.2776196 ], loss 0.3654\n",
      "Iter 13: w [ 1.27488277 -0.25718487], loss 0.3608\n",
      "Iter 14: w [ 1.31565518 -0.23854764], loss 0.3569\n",
      "Iter 15: w [ 1.35310301 -0.22147849], loss 0.3537\n",
      "Iter 16: w [ 1.38766146 -0.20578848], loss 0.3509\n",
      "Iter 17: w [ 1.41968516 -0.19131994], loss 0.3485\n",
      "Iter 18: w [ 1.44946698 -0.17793989], loss 0.3465\n",
      "Iter 19: w [ 1.47725168 -0.16553508], loss 0.3447\n",
      "Iter 20: w [ 1.50324598 -0.1540082 ], loss 0.3431\n",
      "Iter 21: w [ 1.5276261  -0.14327502], loss 0.3417\n",
      "Iter 22: w [ 1.55054357 -0.13326212], loss 0.3405\n",
      "Iter 23: w [ 1.57212966 -0.12390515], loss 0.3394\n",
      "Iter 24: w [ 1.59249885 -0.11514736], loss 0.3385\n",
      "Iter 25: w [ 1.61175166 -0.10693845], loss 0.3376\n",
      "Iter 26: w [ 1.62997681 -0.09923367], loss 0.3369\n",
      "Iter 27: w [ 1.64725304 -0.09199303], loss 0.3362\n",
      "Iter 28: w [ 1.66365057 -0.08518062], loss 0.3356\n",
      "Iter 29: w [ 1.6792323  -0.07876415], loss 0.3350\n",
      "Iter 30: w [ 1.69405479 -0.07271444], loss 0.3345\n",
      "Iter 31: w [ 1.70816912 -0.06700505], loss 0.3341\n",
      "Iter 32: w [ 1.72162158 -0.06161196], loss 0.3337\n",
      "Iter 33: w [ 1.73445423 -0.05651332], loss 0.3333\n",
      "Iter 34: w [ 1.74670544 -0.05168916], loss 0.3329\n",
      "Iter 35: w [ 1.7584103  -0.04712122], loss 0.3326\n",
      "Iter 36: w [ 1.769601   -0.04279275], loss 0.3324\n",
      "Iter 37: w [ 1.7803071  -0.03868837], loss 0.3321\n",
      "Iter 38: w [ 1.79055589 -0.03479393], loss 0.3319\n",
      "Iter 39: w [ 1.80037254 -0.03109638], loss 0.3317\n",
      "Iter 40: w [ 1.80978036 -0.02758366], loss 0.3315\n",
      "Iter 41: w [ 1.81880097 -0.02424463], loss 0.3313\n",
      "Iter 42: w [ 1.82745446 -0.02106895], loss 0.3311\n",
      "Iter 43: w [ 1.83575953 -0.01804707], loss 0.3310\n",
      "Iter 44: w [ 1.84373363 -0.01517008], loss 0.3308\n",
      "Iter 45: w [ 1.85139304 -0.0124297 ], loss 0.3307\n",
      "Iter 46: w [ 1.85875301 -0.00981825], loss 0.3306\n",
      "Iter 47: w [ 1.86582779 -0.00732854], loss 0.3305\n",
      "Iter 48: w [ 1.87263079 -0.00495388], loss 0.3304\n",
      "Iter 49: w [ 1.87917458 -0.00268801], loss 0.3303\n"
     ]
    }
   ],
   "source": [
    "## An one-mode example\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Generate random samples\n",
    "np.random.seed(0)\n",
    "category1 = np.random.normal(loc=1, scale=1, size=50)\n",
    "category2 = np.random.normal(loc=-1, scale=1, size=50)\n",
    "# category2 = np.concatenate([np.random.normal(loc=-1, scale=1, size=50), np.random.normal(loc=-3, scale=1, size=50)])\n",
    "\n",
    "# Prepare data for logistic regression\n",
    "X = np.append(category1, category2).reshape(-1, 1)\n",
    "X = np.hstack((X, np.ones((X.shape[0], 1))))  # Add bias term\n",
    "y = np.append(np.ones(category1.shape[0]), np.zeros(category2.shape[0])) \n",
    "# We are using 0-1 label here due to the usage of binary cross-entropy loss\n",
    "\n",
    "# Weight record\n",
    "weights_history = []\n",
    "\n",
    "\n",
    "# Logistic Regression Functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss(w, X, y, lambda_reg = 0.0):\n",
    "    z = X.dot(w)\n",
    "    predictions = sigmoid(z)\n",
    "    return -np.mean(y * np.log(predictions+eps) + (1 - y) * np.log(1 - predictions+eps)) + 1/2*lambda_reg*w.T.dot(w)\n",
    "\n",
    "def logistic_regression(X, y, lr=0.01, epochs=100, lambda_reg = 0.0):\n",
    "    weights_history = []\n",
    "    w = -np.ones(X.shape[1])\n",
    "    for epoch in range(epochs):\n",
    "        z = np.dot(X, w)\n",
    "        predictions = sigmoid(z)\n",
    "        gradient = np.dot(X.T, (predictions - y)) / y.size + lambda_reg * w\n",
    "        w -= lr * gradient\n",
    "        weights_history.append(w.copy())\n",
    "    return weights_history\n",
    "\n",
    "\n",
    "# Animation function\n",
    "def create_fig():\n",
    "    # Set up the figure for animation with a 16:9 aspect ratio\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    ax1 = fig.add_subplot(131)\n",
    "    ax2 = fig.add_subplot(132, projection='3d')\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    \n",
    "    # Probability Curve\n",
    "    ax1.scatter(category1, np.ones(category1.shape[0]), s = 5, label='Category 1')\n",
    "    ax1.scatter(category2, -np.ones(category2.shape[0]), s = 5, label='Category 2')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('Prediction')\n",
    "    ax1.set_title('Probability Curve')\n",
    "    ax1.grid(True)\n",
    "    x_values = np.linspace(-6, 6, 300)\n",
    "    line, = ax1.plot([], [], color='red', lw=2, label='p(y=1|x; w)')\n",
    "    line2, = ax1.plot([], [], color='blue', lw=2, label='p(y=-1|x; w)')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # 3D Loss\n",
    "    w1_range = np.linspace(-10, 10, 100)\n",
    "    w2_range = np.linspace(-10, 10, 100)\n",
    "    W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "    losses = np.array([compute_loss(np.array([w1, w2]), X, y) for w1, w2 in zip(np.ravel(W1), np.ravel(W2))])\n",
    "    losses = losses.reshape(W1.shape)\n",
    "    ax2.plot_surface(W1, W2, losses, cmap=cm.viridis, edgecolor='none')\n",
    "    ax2.set_xlabel('w1')\n",
    "    ax2.set_ylabel('w2')\n",
    "    ax2.set_zlabel('Loss')\n",
    "    ax2.set_title('Loss Function Landscape')\n",
    "    scat3d = ax2.scatter([], [], [], color='red', s=50)\n",
    "    \n",
    "    # 2D Contour and Optimization Traj\n",
    "    ax3.contour(W1, W2, losses, levels=10, linewidths=0.5, colors='k')\n",
    "    contour = ax3.contourf(W1, W2, losses, levels=50, cmap='RdBu_r')\n",
    "    ax3.set_xlabel('w1')\n",
    "    ax3.set_ylabel('w2')\n",
    "    ax3.set_title('Loss Function')\n",
    "    scat2d = ax3.scatter([], [], color='red', s=50)\n",
    "    loss_text = ax3.text(0.5, -9, '', fontsize=12)\n",
    "    return fig, [ax1, ax2, ax3], [x_values, line, line2, scat3d, scat2d, loss_text]\n",
    "\n",
    "def animate(i):\n",
    "    global weights_history, ax1, ax2, ax3\n",
    "\n",
    "    weights = weights_history[i]\n",
    "    w1, w2 = weights\n",
    "    weights_history.append([w1, w2])\n",
    "\n",
    "    # Update probability curve\n",
    "    y_values = sigmoid(x_values * w1 + w2)\n",
    "    line.set_data(x_values, y_values)\n",
    "    line2.set_data(x_values, 1-y_values)\n",
    "\n",
    "\n",
    "    # Update 3D loss function plot\n",
    "    scat3d._offsets3d = ([w1], [w2], [compute_loss(weights, X, y)+1]) # add offset to make the dot visible\n",
    "    \n",
    "    # Update 2D contour plot\n",
    "    loss_val = compute_loss(weights, X, y)\n",
    "    scat2d.set_offsets([w1, w2])\n",
    "    ax3.plot(*zip(*weights_history[:i+1]), color='red', markersize=1)\n",
    "    loss_text.set_text(f'Iter: {i:d} Loss: {loss_val:.4f}')\n",
    "    print(f'Iter {i:d}: w {weights}, loss {loss_val:.4f}')\n",
    "    return line, scat3d, scat2d, loss_text\n",
    "\n",
    "\n",
    "eps = 1e-15\n",
    "\n",
    "# Initialize weights generator\n",
    "weights_history = logistic_regression(X, y, lr=.5, epochs=100)\n",
    "\n",
    "# Create animation\n",
    "fig, [ax1, ax2, ax3], [x_values, line, line2, scat3d, scat2d, loss_text] = create_fig()\n",
    "ani = FuncAnimation(fig, animate, frames=50, interval=100, blit=True)\n",
    "\n",
    "# Save the animation\n",
    "ani.save('logistic_regression_training.mp4', writer='ffmpeg', dpi=300, fps=5)\n",
    "\n",
    "plt.close(fig)  # Close the figure after saving the animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79ed86d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: w [12.44645071  1.59066931], loss 1.0010\n",
      "Iter 0: w [12.44645071  1.59066931], loss 1.0010\n",
      "Iter 0: w [12.44645071  1.59066931], loss 1.0010\n",
      "Iter 0: w [12.44645071  1.59066931], loss 1.0010\n",
      "Iter 1: w [11.32183566  1.40742464], loss 0.9148\n",
      "Iter 2: w [10.20313485  1.24005495], loss 0.8298\n",
      "Iter 3: w [9.09313014 1.08719153], loss 0.7467\n",
      "Iter 4: w [7.99601019 0.94639169], loss 0.6658\n",
      "Iter 5: w [6.91830728 0.81387726], loss 0.5882\n",
      "Iter 6: w [5.87054266 0.68448487], loss 0.5154\n",
      "Iter 7: w [4.87007146 0.55242591], loss 0.4498\n",
      "Iter 8: w [3.94582688 0.41410261], loss 0.3950\n",
      "Iter 9: w [3.14531653 0.27433727], loss 0.3557\n",
      "Iter 10: w [2.53906413 0.15341158], loss 0.3352\n",
      "Iter 11: w [2.19433897 0.07977071], loss 0.3295\n",
      "Iter 12: w [2.08217823 0.05574279], loss 0.3290\n",
      "Iter 13: w [2.06581388 0.05209579], loss 0.3290\n",
      "Iter 14: w [2.06441739 0.05184466], loss 0.3290\n",
      "Iter 15: w [2.06431883 0.05179369], loss 0.3290\n",
      "Iter 16: w [2.06430753 0.05180794], loss 0.3290\n",
      "Iter 17: w [2.06430891 0.05179879], loss 0.3290\n",
      "Iter 18: w [2.06430775 0.05180388], loss 0.3290\n",
      "Iter 19: w [2.06430838 0.051801  ], loss 0.3290\n",
      "Iter 20: w [2.06430802 0.05180263], loss 0.3290\n",
      "Iter 21: w [2.06430822 0.05180171], loss 0.3290\n",
      "Iter 22: w [2.06430811 0.05180223], loss 0.3290\n",
      "Iter 23: w [2.06430817 0.05180193], loss 0.3290\n",
      "Iter 24: w [2.06430814 0.0518021 ], loss 0.3290\n",
      "Iter 25: w [2.06430816 0.051802  ], loss 0.3290\n",
      "Iter 26: w [2.06430815 0.05180206], loss 0.3290\n",
      "Iter 27: w [2.06430815 0.05180203], loss 0.3290\n",
      "Iter 28: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 29: w [2.06430815 0.05180203], loss 0.3290\n",
      "Iter 30: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 31: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 32: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 33: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 34: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 35: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 36: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 37: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 38: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 39: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 40: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 41: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 42: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 43: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 44: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 45: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 46: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 47: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 48: w [2.06430815 0.05180204], loss 0.3290\n",
      "Iter 49: w [2.06430815 0.05180204], loss 0.3290\n"
     ]
    }
   ],
   "source": [
    "## What if we use a very large learning rate?\n",
    "\n",
    "# Initialize weights generator\n",
    "weights_history = logistic_regression(X, y, lr=15, epochs=100)\n",
    "\n",
    "# Create animation\n",
    "fig, [ax1, ax2, ax3], [x_values, line, line2, scat3d, scat2d, loss_text] = create_fig()\n",
    "ani = FuncAnimation(fig, animate, frames=50, interval=100, blit=True)\n",
    "\n",
    "# Save the animation\n",
    "ani.save('logistic_regression_training_large_l1.mp4', writer='ffmpeg', dpi=300, fps=5)\n",
    "\n",
    "plt.close(fig)  # Close the figure after saving the animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f59d4253-9799-45df-adcd-172d3d87cea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 0: w [ 1.12537604 -0.53050328], loss 0.2226\n",
      "Iter 0: w [ 1.12537604 -0.53050328], loss 0.2226\n",
      "Iter 0: w [ 1.12537604 -0.53050328], loss 0.2226\n",
      "Iter 0: w [ 1.12537604 -0.53050328], loss 0.2226\n",
      "Iter 1: w [ 1.29145316 -0.47712333], loss 0.1949\n",
      "Iter 2: w [ 1.43081901 -0.4377318 ], loss 0.1754\n",
      "Iter 3: w [ 1.55248219 -0.40806117], loss 0.1606\n",
      "Iter 4: w [ 1.66106324 -0.38555442], loss 0.1490\n",
      "Iter 5: w [ 1.75941251 -0.3685026 ], loss 0.1395\n",
      "Iter 6: w [ 1.84947354 -0.3556981 ], loss 0.1315\n",
      "Iter 7: w [ 1.93265465 -0.34625709], loss 0.1248\n",
      "Iter 8: w [ 2.01001872 -0.33951608], loss 0.1189\n",
      "Iter 9: w [ 2.08239196 -0.3349669 ], loss 0.1138\n",
      "Iter 10: w [ 2.15043143 -0.3322139 ], loss 0.1093\n",
      "Iter 11: w [ 2.21466925 -0.33094461], loss 0.1053\n",
      "Iter 12: w [ 2.27554307 -0.33090929], loss 0.1017\n",
      "Iter 13: w [ 2.33341747 -0.33190614], loss 0.0984\n",
      "Iter 14: w [ 2.38859962 -0.33377058], loss 0.0955\n",
      "Iter 15: w [ 2.44135089 -0.33636721], loss 0.0927\n",
      "Iter 16: w [ 2.4918956  -0.33958381], loss 0.0902\n",
      "Iter 17: w [ 2.54042783 -0.34332673], loss 0.0879\n",
      "Iter 18: w [ 2.58711662 -0.34751733], loss 0.0857\n",
      "Iter 19: w [ 2.63211019 -0.35208921], loss 0.0837\n",
      "Iter 20: w [ 2.67553924 -0.35698601], loss 0.0818\n",
      "Iter 21: w [ 2.71751961 -0.36215969], loss 0.0801\n",
      "Iter 22: w [ 2.75815453 -0.36756909], loss 0.0784\n",
      "Iter 23: w [ 2.79753634 -0.37317886], loss 0.0769\n",
      "Iter 24: w [ 2.83574803 -0.3789585 ], loss 0.0754\n",
      "Iter 25: w [ 2.87286443 -0.38488162], loss 0.0740\n",
      "Iter 26: w [ 2.90895331 -0.39092532], loss 0.0727\n",
      "Iter 27: w [ 2.94407618 -0.39706968], loss 0.0714\n",
      "Iter 28: w [ 2.97828912 -0.40329732], loss 0.0702\n",
      "Iter 29: w [ 3.01164334 -0.40959307], loss 0.0691\n",
      "Iter 30: w [ 3.04418577 -0.41594362], loss 0.0680\n",
      "Iter 31: w [ 3.07595952 -0.42233733], loss 0.0670\n",
      "Iter 32: w [ 3.10700429 -0.42876396], loss 0.0660\n",
      "Iter 33: w [ 3.13735671 -0.43521449], loss 0.0650\n",
      "Iter 34: w [ 3.16705067 -0.44168101], loss 0.0641\n",
      "Iter 35: w [ 3.19611756 -0.44815653], loss 0.0632\n",
      "Iter 36: w [ 3.22458655 -0.4546349 ], loss 0.0624\n",
      "Iter 37: w [ 3.25248478 -0.46111068], loss 0.0616\n",
      "Iter 38: w [ 3.27983752 -0.46757909], loss 0.0608\n",
      "Iter 39: w [ 3.30666839 -0.4740359 ], loss 0.0600\n",
      "Iter 40: w [ 3.33299944 -0.48047739], loss 0.0593\n",
      "Iter 41: w [ 3.35885135 -0.48690028], loss 0.0586\n",
      "Iter 42: w [ 3.3842435  -0.49330168], loss 0.0579\n",
      "Iter 43: w [ 3.4091941  -0.49967905], loss 0.0572\n",
      "Iter 44: w [ 3.43372024 -0.50603014], loss 0.0566\n",
      "Iter 45: w [ 3.45783807 -0.51235302], loss 0.0560\n",
      "Iter 46: w [ 3.48156275 -0.51864597], loss 0.0554\n",
      "Iter 47: w [ 3.50490864 -0.52490749], loss 0.0548\n",
      "Iter 48: w [ 3.52788928 -0.5311363 ], loss 0.0543\n",
      "Iter 49: w [ 3.55051749 -0.53733128], loss 0.0537\n",
      "Iter 50: w [ 3.57280541 -0.54349147], loss 0.0532\n",
      "Iter 51: w [ 3.59476453 -0.54961604], loss 0.0527\n",
      "Iter 52: w [ 3.61640577 -0.5557043 ], loss 0.0522\n",
      "Iter 53: w [ 3.6377395  -0.56175567], loss 0.0517\n",
      "Iter 54: w [ 3.65877556 -0.56776967], loss 0.0512\n",
      "Iter 55: w [ 3.67952334 -0.57374591], loss 0.0507\n",
      "Iter 56: w [ 3.69999174 -0.57968408], loss 0.0503\n",
      "Iter 57: w [ 3.72018929 -0.58558394], loss 0.0498\n",
      "Iter 58: w [ 3.74012408 -0.59144534], loss 0.0494\n",
      "Iter 59: w [ 3.75980386 -0.59726815], loss 0.0490\n",
      "Iter 60: w [ 3.77923602 -0.60305232], loss 0.0486\n",
      "Iter 61: w [ 3.79842763 -0.60879784], loss 0.0482\n",
      "Iter 62: w [ 3.81738544 -0.61450474], loss 0.0478\n",
      "Iter 63: w [ 3.83611594 -0.62017309], loss 0.0474\n",
      "Iter 64: w [ 3.85462531 -0.62580299], loss 0.0470\n",
      "Iter 65: w [ 3.87291949 -0.63139459], loss 0.0467\n",
      "Iter 66: w [ 3.89100417 -0.63694803], loss 0.0463\n",
      "Iter 67: w [ 3.90888483 -0.64246351], loss 0.0460\n",
      "Iter 68: w [ 3.9265667  -0.64794124], loss 0.0456\n",
      "Iter 69: w [ 3.94405483 -0.65338143], loss 0.0453\n",
      "Iter 70: w [ 3.96135406 -0.65878434], loss 0.0450\n",
      "Iter 71: w [ 3.97846904 -0.66415022], loss 0.0447\n",
      "Iter 72: w [ 3.99540427 -0.66947934], loss 0.0443\n",
      "Iter 73: w [ 4.01216405 -0.67477198], loss 0.0440\n",
      "Iter 74: w [ 4.02875254 -0.68002843], loss 0.0437\n",
      "Iter 75: w [ 4.04517374 -0.685249  ], loss 0.0434\n",
      "Iter 76: w [ 4.06143153 -0.690434  ], loss 0.0431\n",
      "Iter 77: w [ 4.07752961 -0.69558373], loss 0.0429\n",
      "Iter 78: w [ 4.09347159 -0.70069852], loss 0.0426\n",
      "Iter 79: w [ 4.10926093 -0.70577869], loss 0.0423\n",
      "Iter 80: w [ 4.12490098 -0.71082457], loss 0.0420\n",
      "Iter 81: w [ 4.14039499 -0.71583648], loss 0.0418\n",
      "Iter 82: w [ 4.15574608 -0.72081477], loss 0.0415\n",
      "Iter 83: w [ 4.17095727 -0.72575977], loss 0.0413\n",
      "Iter 84: w [ 4.1860315  -0.73067181], loss 0.0410\n",
      "Iter 85: w [ 4.20097159 -0.73555122], loss 0.0408\n",
      "Iter 86: w [ 4.21578029 -0.74039834], loss 0.0405\n",
      "Iter 87: w [ 4.23046025 -0.74521352], loss 0.0403\n",
      "Iter 88: w [ 4.24501405 -0.74999707], loss 0.0401\n",
      "Iter 89: w [ 4.25944418 -0.75474934], loss 0.0398\n",
      "Iter 90: w [ 4.27375306 -0.75947065], loss 0.0396\n",
      "Iter 91: w [ 4.28794303 -0.76416134], loss 0.0394\n",
      "Iter 92: w [ 4.30201637 -0.76882174], loss 0.0392\n",
      "Iter 93: w [ 4.31597528 -0.77345216], loss 0.0389\n",
      "Iter 94: w [ 4.3298219  -0.77805295], loss 0.0387\n",
      "Iter 95: w [ 4.34355832 -0.78262441], loss 0.0385\n",
      "Iter 96: w [ 4.35718656 -0.78716686], loss 0.0383\n",
      "Iter 97: w [ 4.37070858 -0.79168063], loss 0.0381\n",
      "Iter 98: w [ 4.38412628 -0.79616603], loss 0.0379\n",
      "Iter 99: w [ 4.39744153 -0.80062336], loss 0.0377\n"
     ]
    }
   ],
   "source": [
    "## A two-mode example\n",
    "import numpy as np\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation, PillowWriter\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Generate random samples\n",
    "np.random.seed(0)\n",
    "category1 = np.concatenate([np.random.normal(loc=1, scale=.25, size=50), np.random.normal(loc=5, scale=.25, size=50)])\n",
    "category2 = np.random.normal(loc=-1, scale=.5, size=50)\n",
    "\n",
    "# Prepare data for logistic regression\n",
    "X = np.append(category1, category2).reshape(-1, 1)\n",
    "X = np.hstack((X, np.ones((X.shape[0], 1))))  # Add bias term\n",
    "y = np.append(np.ones(category1.shape[0]), np.zeros(category2.shape[0])) \n",
    "# We are using 0-1 label here due to the usage of binary cross-entropy loss\n",
    "\n",
    "# Weight record\n",
    "weights_history = []\n",
    "\n",
    "\n",
    "# Logistic Regression Functions\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def compute_loss(w, X, y, lambda_reg = 0.0):\n",
    "    z = X.dot(w)\n",
    "    predictions = sigmoid(z)\n",
    "    return -np.mean(y * np.log(predictions+eps) + (1 - y) * np.log(1 - predictions+eps)) + 1/2*lambda_reg*w.T.dot(w)\n",
    "\n",
    "def logistic_regression(X, y, lr=0.01, epochs=100, lambda_reg = 0.0):\n",
    "    weights_history = []\n",
    "    w = -np.ones(X.shape[1])\n",
    "    for epoch in range(epochs):\n",
    "        z = np.dot(X, w)\n",
    "        predictions = sigmoid(z)\n",
    "        gradient = np.dot(X.T, (predictions - y)) / y.size + lambda_reg * w\n",
    "        w -= lr * gradient\n",
    "        weights_history.append(w.copy())\n",
    "    return weights_history\n",
    "\n",
    "\n",
    "# Animation function\n",
    "def create_fig():\n",
    "    # Set up the figure for animation with a 18:6 aspect ratio\n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "    ax1 = fig.add_subplot(131)\n",
    "    ax2 = fig.add_subplot(132, projection='3d')\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    \n",
    "    # Probability Curve\n",
    "    ax1.scatter(category1, np.ones(category1.shape[0]), s = 5, label='Category 1')\n",
    "    ax1.scatter(category2, -np.ones(category2.shape[0]), s = 5, label='Category 2')\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('Prediction')\n",
    "    ax1.set_title('Probability Curve')\n",
    "    ax1.grid(True)\n",
    "    x_values = np.linspace(-6, 6, 300)\n",
    "    line, = ax1.plot([], [], color='red', lw=2, label='p(y=1|x; w)')\n",
    "    line2, = ax1.plot([], [], color='blue', lw=2, label='p(y=-1|x; w)')\n",
    "    ax1.legend()\n",
    "    \n",
    "    \n",
    "    # 3D Loss\n",
    "    w1_range = np.linspace(-10, 10, 100)\n",
    "    w2_range = np.linspace(-10, 10, 100)\n",
    "    W1, W2 = np.meshgrid(w1_range, w2_range)\n",
    "    losses = np.array([compute_loss(np.array([w1, w2]), X, y) for w1, w2 in zip(np.ravel(W1), np.ravel(W2))])\n",
    "    losses = losses.reshape(W1.shape)\n",
    "    ax2.plot_surface(W1, W2, losses, cmap=cm.viridis, edgecolor='none')\n",
    "    ax2.set_xlabel('w1')\n",
    "    ax2.set_ylabel('w2')\n",
    "    ax2.set_zlabel('Loss')\n",
    "    ax2.set_title('Loss Function Landscape')\n",
    "    scat3d = ax2.scatter([], [], [], color='red', s=50)\n",
    "    \n",
    "    # 2D Contour and Optimization Traj\n",
    "    ax3.contour(W1, W2, losses, levels=10, linewidths=0.5, colors='k')\n",
    "    contour = ax3.contourf(W1, W2, losses, levels=50, cmap='RdBu_r')\n",
    "    ax3.set_xlabel('w1')\n",
    "    ax3.set_ylabel('w2')\n",
    "    ax3.set_title('Loss Function')\n",
    "    scat2d = ax3.scatter([], [], color='red', s=50)\n",
    "    loss_text = ax3.text(0.5, -9, '', fontsize=12)\n",
    "    return fig, [ax1, ax2, ax3], [x_values, line, line2, scat3d, scat2d, loss_text]\n",
    "\n",
    "def animate(i):\n",
    "    global weights, ax1, ax2, ax3\n",
    "\n",
    "    weights = weights_history[i]\n",
    "    w1, w2 = weights\n",
    "    weights_history.append([w1, w2])\n",
    "\n",
    "    # Update probability curve\n",
    "    y_values = sigmoid(x_values * w1 + w2)\n",
    "    line.set_data(x_values, y_values)\n",
    "    line2.set_data(x_values, 1-y_values)\n",
    "\n",
    "\n",
    "    # Update 3D loss function plot\n",
    "    scat3d._offsets3d = ([w1], [w2], [compute_loss(weights, X, y)+1]) # add offset to make the dot visible\n",
    "    \n",
    "    # Update 2D contour plot\n",
    "    loss_val = compute_loss(weights, X, y)\n",
    "    scat2d.set_offsets([w1, w2])\n",
    "    ax3.plot(*zip(*weights_history[:i+1]), color='red', markersize=1)\n",
    "    loss_text.set_text(f'Iter: {i:d} Loss: {loss_val:.4f}')\n",
    "    print(f'Iter {i:d}: w {weights}, loss {loss_val:.4f}')\n",
    "    return line, scat3d, scat2d, loss_text\n",
    "\n",
    "\n",
    "eps = 1e-15\n",
    "\n",
    "# Initialize weights generator\n",
    "weights_history = logistic_regression(X, y, lr=1, epochs=200)\n",
    "\n",
    "# Create animation\n",
    "fig, [ax1, ax2, ax3], [x_values, line, line2, scat3d, scat2d, loss_text] = create_fig()\n",
    "ani = FuncAnimation(fig, animate, frames=100, interval=100, blit=True)\n",
    "\n",
    "# Save the animation\n",
    "ani.save('logistic_regression_training_two_mode.mp4', writer='ffmpeg', dpi=300, fps=5)\n",
    "\n",
    "plt.close(fig)  # Close the figure after saving the animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29efa1d7-3492-4bbf-ad74-4c1092dd11f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

\documentclass[12pt]{article}
\usepackage{homework}

\onehalfspacing
\graphicspath{{images/}}
\geometry{letterpaper, portrait, includeheadfoot=true, hmargin=1in, vmargin=1in}

\setcounter{section}{-1}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Solution hiding %%
\usepackage[utf8]{inputenc}
\usepackage{lipsum}

\usepackage{ifthen}
\newboolean{hidesoln}
\setboolean{hidesoln}{false} 
\newboolean{hiderubric}
\setboolean{hiderubric}{false}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% following loops stolen from djhsu
\def\ddefloop#1{\ifx\ddefloop#1\else\ddef{#1}\expandafter\ddefloop\fi}
\def\ddef#1{\expandafter\def\csname bb#1\endcsname{\ensuremath{\mathbb{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop
\def\ddef#1{\expandafter\def\csname c#1\endcsname{\ensuremath{\mathcal{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop
\def\ddef#1{\expandafter\def\csname bf#1\endcsname{\ensuremath{\mathbf{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\ddefloop
% \cA, \cB, ...
\def\ddef#1{\expandafter\def\csname c#1\endcsname{\ensuremath{\mathcal{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZ\ddefloop

% \vA, \vB, ..., \va, \vb, ...
\def\ddef#1{\expandafter\def\csname v#1\endcsname{\ensuremath{\boldsymbol{#1}}}}
\ddefloop ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\ddefloop
% \valpha, \vbeta, ...,  \vGamma, \vDelta, ...,
\def\ddef#1{\expandafter\def\csname v#1\endcsname{\ensuremath{\boldsymbol{\csname #1\endcsname}}}}
\ddefloop {alpha}{beta}{gamma}{delta}{epsilon}{varepsilon}{zeta}{eta}{theta}{vartheta}{iota}{kappa}{lambda}{mu}{nu}{xi}{pi}{varpi}{rho}{varrho}{sigma}{varsigma}{tau}{upsilon}{phi}{varphi}{chi}{psi}{omega}{Gamma}{Delta}{Theta}{Lambda}{Xi}{Pi}{Sigma}{varSigma}{Upsilon}{Phi}{Psi}{Omega}{ell}\ddefloop

\def\SPAN{\textup{span}}
\def\tu{\textup{u}}
\def\R{\mathbb{R}}
\def\E{\mathbb{E}}
\def\Z{\mathbb{Z}}
\def\be{\mathbf{e}}
\def\nf{\nabla f}
\def\veps{\varepsilon}
\def\cl{\textup{cl}}
\def\inte{\textup{int}}
\def\dom{\textup{dom}}
\def\Rad{\textup{Rad}}
\def\lsq{\ell_{\textup{sq}}}
\def\hcR{\widehat{\cR}}
\def\hcRl{\hcR_\ell}
\def\cRl{\cR_\ell}
\def\hcE{\widehat{\cE}}
\def\cEl{\cE_\ell}
\def\hcEl{\hcE_\ell}
\def\eps{\epsilon}
\def\1{\mathds{1}}
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\blue}[1]{{\color{blue} #1}}
\def\srelu{\sigma_{\textup{r}}}
\def\vsrelu{\vec{\sigma_{\textup{r}}}}
\def\vol{\textup{vol}}
\def\tF{{\scriptscriptstyle\textup{F}}}
\DeclareMathOperator{\tr}{tr}
\newcommand\T{{\scriptscriptstyle\mathsf{T}}}

\newcommand{\ip}[2]{\left\langle #1, #2 \right \rangle}
\newcommand{\mjt}[1]{{\color{blue}\emph\textbf{[M:}~#1~\textbf{]}}}

\def\hPr{\widehat{\textup{Pr}}}
\def\Lip{\textup{Lip}}

\begin{document}
\singlespacing

\renewcommand{\familydefault}{\rmdefault}
\input{headerfooter}

\section{Instructions}

Homework is due Tuesday, April 2, 2024 at 23:59pm Central Time.
Please refer to \url{https://courses.grainger.illinois.edu/cs446/sp2024/homework/hw/index.html} for course policy on homeworks and submission instructions.

\ifthenelse{\boolean{hidesoln}}{}{
    \textbf{Reminder:} Answers must be typeset. \LaTeX  and other methods of typesetting math are accepted.
}

\section{PCA: 6pts}
\begin{enumerate}

\item (1pts) Recall that PCA finds a direction $w$ in which the projected data has highest variance by solving the following program:
    \begin{equation}
        \max_{w:||w||^2=1}w^T\Sigma w.
        \label{equ:pca}
    \end{equation}
Here, $\Sigma$ is a covariance matrix. You are given a dataset of two 2-dimensional points $(1,3)$ and $(4,7)$. Draw the two data points on the 2D plane. What is the first principal component $w$ of this dataset?

\item (3pts) Now you are given a dataset of four points $(2,0)$, $(2,2)$, $(6,0)$ and $(6,2)$. Given this dataset, derive the covariance matrix $\Sigma$ in Eq.\ref{equ:pca}. Then plot the centralized data with the first and the second principal components in one figure. \textbf{Include the plot in your written submission}.

\item (2pts) What is the optimal $w$ and the optimal value of the program in Eq.\ref{equ:pca}  given \[ \Sigma= \left[ \begin{array}{cccc}
	12 & 0 & 0 & 0\\
	0 & 6 & 0 & 0\\
	0 & 0 & 20 & 0\\
	0 & 0 & 0 & 10\\
	\end{array} \right].\] Give your justification.


\end{enumerate}
    
\section{Basics in Information Theory: 7pts}
Let $X$ be a discrete variable, and $P$, $Q$ be two probability distributions over $X$. Define a new random variable $X'$ as follows:
\begin{equation*}
    X' = \begin{cases}
        X \sim P & \text{if } B=1, \\
        X \sim Q & \text{if } B=0,
    \end{cases}
\end{equation*}
where $B \in \{0,1\}$ is an independent and Bernoulli distribution over $\{0,1\}$ with the parameter $\lambda$, such that $\Pr(B=1) = \lambda = 1-\Pr(B=0)$.

\begin{enumerate}
    \item (2pts) Derive and represent the mixture distribution $\Pr(X' = x)$ in terms of $P(x)$ and $Q(x)$.

    \item (5pts) Show that $I(X';B)=D_{\lambda}(P\|Q)$, where $D_{\lambda}(P\|Q)$ is the $\lambda$-divergence between $P$ and $Q$, i.e., $D_{\lambda}(P\|Q) = \lambda D_{\text{KL}}(P\| \lambda P + (1-\lambda) Q) + (1-\lambda) D_{\text{KL}}(Q \| \lambda P + (1-\lambda) Q)$. Note that by setting $\lambda = 0.5$, the $\lambda$-divergence gives the Jensen-Shannon divergence.

\end{enumerate}

\section{k-Means with Soft Assignments: 10pts}
Consider the following exemplar-based, hard-assignment form as the objective of k-Means for $K$ clusters and $n$ data points $x^{(i)}$ for $i = 1, ..., n$:
\begin{align}
    \label{kmeans}
    \underset{\mu_1, ..., \mu_K}{\min}\sum_{i=1}^n \underset{k}{\min} \|x^{(i)}-\mu_k\|_2^2 = \underset{\mu_1, ..., \mu_K}{\min}\underset{\substack{A \in \{0,1\}^{n\times K} \\ A\cdot\mathbf{1}_K= \mathbf{1}_n}}{\min}\sum_{i=1}^n\sum_{k=1}^K A_{ik}\|x^{(i)} - \mu_k\|^2_2, 
\end{align}
where $\mu_k$ denotes the center for the $k$-th cluster, the matrix $A \in \{0,1\}^{n\times K}$ indicates the hard assignment of each data point to the clusters, and $A\cdot \mathbf{1}_K = \mathbf{1}_n$, which tells us that each row of $A$ has one $1$ with all remaining elements as $0$, i,e, $\sum_{k=1}^K A_{ik} = 1, \forall i$.

We extend this setting to soft assignment by designing the matrix $A \in [0,1]^{n\times k}$, and the objective becomes:
\begin{align}
    \label{soft}
    \underset{\mu_1, ..., \mu_K}{\min}\underset{\substack{A \in [0,1]^{n\times K} \\ A\cdot\mathbf{1}_K= \mathbf{1}_n}}{\min}\sum_{i=1}^n\sum_{k=1}^K A_{ik}\|x^{(i)} - \mu_k\|^2_2. 
\end{align}

\begin{enumerate}
    \item (3pts) Show that the following holds:
    \begin{align}
        \underset{\mu_1, ..., \mu_K}{\min}\underset{\substack{A \in [0,1]^{n\times K} \\ A\cdot\mathbf{1}_K= \mathbf{1}_n}}{\min}\sum_{i=1}^n\sum_{k=1}^K A_{ik}\|x^{(i)} - \mu_k\|^2_2 \leq \underset{\mu_1, ..., \mu_K}{\min}\underset{\substack{A \in \{0,1\}^{n\times K} \\ A\cdot\mathbf{1}_K= \mathbf{1}_n}}{\min}\sum_{i=1}^n\sum_{k=1}^K A_{ik}\|x^{(i)} - \mu_k\|^2_2\notag
    \end{align}
    
    \textbf{Hint:} Note that $\{0,1\}^{n\times K}$ can be seen as a subset of $[0,1]^{n\times K}$.

    \item (5pts) Show that the following also holds:
    \begin{align}
        \underset{\mu_1, ..., \mu_K}{\min}\underset{\substack{A \in [0,1]^{n\times K} \\ A\cdot\mathbf{1}_K= \mathbf{1}_n}}{\min}\sum_{i=1}^n\sum_{k=1}^K A_{ik}\|x^{(i)} - \mu_k\|^2_2 \geq \underset{\mu_1, ..., \mu_K}{\min}\underset{\substack{A \in \{0,1\}^{n\times K} \\ A\cdot\mathbf{1}_K= \mathbf{1}_n}}{\min}\sum_{i=1}^n\sum_{k=1}^K A_{ik}\|x^{(i)} - \mu_k\|^2_2\notag
    \end{align}
    
    \textbf{Hint:} You may use the fact that $\|x^{(i)} - \mu_k\|^2_2 \geq \underset{l}{\min}\|x^{(i)} - \mu_l\|^2_2$, for any $i$ and $k$.

    \item (2pts) Show that the soft assignment problem introduced in this problem (Eq. \ref{soft}) corresponds to a globally optimal hard assignment.
    
\end{enumerate}


\section{Bernoulli Mixture Model: 18pts}
Extended from the Gaussian mixture model introduced in the lecture, we explore the Bernoulli mixture model in this problem. We represent the dataset as $X \in \{0,1\}^{n\times d}$ and each data instance is a set of $d$ independent binary random variables $x^{(i)} = \{x^{(i)}_1, x^{(i)}_2, ..., x^{(i)}_d\}$ and the probability that $x^{(i)}$ is generated from the $k$-th Bernoulli distributions is calculated as:
\begin{align}
    \Pr(x^{(i)}|\mu_k) = \prod_{j=1}^d \mu_k^{x^{(i)}_j}(1-\mu_k)^{\left(1-x^{(i)}_j\right)} ,\notag
\end{align}
where $\mu_k$ is the mean of the $k$-th Bernoulli distribution.

We consider $K$ mixed Bernoulli distributions and introduce the auxiliary/latent variable $z_{ik} \in \{0,1\}$ with $\sum_{k=1}^K z_{ik} = 1 \ \forall i$ as the assignment for $x^{(i)}$ to the $k$-th Bernoulli distribution. Also, we have $\Pr(z_{ik} = 1) = \pi_k$ and $\Pr(x^{(i)}|z_{ik} = 1) = \Pr(x^{(i)}|\mu_k)$.

\begin{enumerate}
    \item (5pts) Derive the log-likelihood $\log \Pr(x^{(i)}, z_i| \pi, \mu)$.

    \item (5pts) In the \textbf{expectation} step, derive the update step for the assignment (posterior) $z_{ik}^{\text{new}} = \Pr(z_{ik} = 1|x^{(i)})$.

    \item (8pts) In the \textbf{maximization} step, derive the update step for the model parameter, i.e., $\mu_k^{\text{new}}$ and $\pi_k^{\text{new}}$.

\end{enumerate}

\section{Variational Autoencoder (VAE): 19pts}
  
In this problem you will implement a Variational Autoencoder (VAE) to model points sampled from an unknown distribution.  This will be done by constructing an encoder network and a decoder network.  The encoder network $f_{\textup{enc}} : X \subset \mathbb{R}^2 \to \mathbb{R}^h \times \mathbb{R}^h$ takes as input a point $\vx$ from the input space and outputs parameters $(\vmu, \vxi)$ where $\vxi =  \log  \vsigma^2$. The decoder network $f_{\textup{dec}} : \mathbb{R}^h \to \mathbb{R}^2$ takes as input a latent vector $\vz \sim \cN(\vmu, \vsigma^2)$ and outputs an element $\hat{\vx} \in \mathbb{R}^2$ that we would hope is similar to members of the input space $X$. You will train this model by minimizing the (regularized) empirical risk
\begin{align*}
 \hcR_{\textup{VAE}} (f) =  \frac{1}{n}\sum_{i=1}^n \ell ( \hat \vx_i , \vx_i) + \lambda \textup{KL}\left(\cN(\vmu(\vx_i), \exp(\vxi(\vx_i)/2)), \cN(0, I)\right).
\end{align*}

Particularly, we have 
\begin{align*}
    \textup{KL}\left(\cN(\vmu, \vSigma ), \cN(0,I)\right) = -\frac{1}{2} \left[ h + \sum_{j=1}^h\left( \log \sigma^2_j - \mu_j^2 - \sigma_j^2 \right) \right],
\end{align*}

\begin{enumerate}

\item (3pts) Use the empirical risk discussed above to implement a VAE in the class \texttt{VAE}.  Use ReLU activations between each layer, except on the last layer of the decoder use sigmoid.  Use the Adam optimizer to optimize in the \texttt{step()} function.  Make use of the PyTorch library for this. Use \texttt{torch.optim.Adam()}, there is no need to implement it yourself.  Please refer to the docstrings in \texttt{hw4.py} for more implementation details.
\item (5pts) Implement the \texttt{fit} function using the \texttt{step()} function from the \texttt{VAE} class.  See the docstrings in \texttt{hw4.py} for more details.  
\item (11pts) Fit a \texttt{VAE} on the data generated by \texttt{generate\_data} in \texttt{hw4\_utils.py}.  Use a learning rate $\eta = 0.01$, latent space dimension $h = 6$, KL-divergence scaling factor $\lambda = 5 \times 10^{-5}$, and train for 8000 iterations. Use least squares as the loss, that is, let $\ell(\vx,\hat\vx) = \Vert \vx - \hat\vx \Vert^2_2$.  \textbf{Include separate plots of each of the following with a legend in your written submission}:
\begin{enumerate}
\item Your empirical risk $\hcR_{\textup{VAE}}$ on the training data vs iteration count;
\item The data points $(\vx)_{i=1}^n$ along with their encoded and decoded approximations $\hat \vx$;
\item The data points $(\vx)_{i=1}^n$ along with their encoded and decoded approximations $\hat \vx$, and $n$ generated points $f_{\textup{dec}}(\vz)$ where $\vz \sim \cN ( 0, I)$.
\end{enumerate}
After you are done training, save your neural network to a checkpoint file using \\
\texttt{torch.save(model.cpu().state\_dict(), "vae.pb")}. \textbf{You will submit this checkpoint file \texttt{"vae.pb"} to the autograder with your code submission.}

 \end{enumerate}

\end{document}
